{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "836de6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyscopus import Scopus\n",
    "import pandas as pd\n",
    "import requests\n",
    "import xmltodict, json\n",
    "\n",
    "\n",
    "from lxml import etree\n",
    "import requests\n",
    "import xmltodict, json\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "96399313",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Ingrese el dato a buscar:\")\n",
    "#busqueda = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6463c90",
   "metadata": {},
   "source": [
    "Prueba Masiva "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e77fee5b",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Declaracion \n",
    "listalinks = list()\n",
    "listatitulos = list()\n",
    "listabstract = list()\n",
    "listautor= list()\n",
    "listakeywords = list()\n",
    "listafuente = list()\n",
    "listkey=list()\n",
    "\n",
    "list_token_palabra_abstract=[]\n",
    "list_token_palabra=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "6cfa84b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_scopus(busqueda):\n",
    "    \n",
    "    \n",
    "    key = '0387c7142db3254ed53c09c131ecf8c0'\n",
    "    scopus = Scopus(key)\n",
    "    search_df = scopus.search(\"ALL(\"+busqueda+\")\", count=100)\n",
    "    for i in range(len(search_df['scopus_id'])):\n",
    "        try:\n",
    "            listalinks.append(\"https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=\"+search_df['scopus_id'][i]+\"&origin=inward\")\n",
    "            try:\n",
    "                pub_abs = scopus.retrieve_abstract(search_df['scopus_id'][i])\n",
    "                listabstract.append(pub_abs['abstract'])\n",
    "            except:\n",
    "                listabstract.append(\"0\")\n",
    "\n",
    "            try:\n",
    "                listatitulos.append(search_df['title'][i])\n",
    "            except:\n",
    "                listatitulos.append(\"0\")\n",
    "\n",
    "            try:\n",
    "                list_aux=[]\n",
    "                for j in range(len(search_df['authors'][i])):\n",
    "                    aut=scopus.retrieve_author(search_df['authors'][i][j])\n",
    "                    list_aux.append(aut['name'])\n",
    "\n",
    "                listautor.append(list_aux)\n",
    "            except:\n",
    "                listautor.append(\"0\")\n",
    "\n",
    "            listakeywords.append(\"0\")\n",
    "            listafuente.append(\"scopus\")\n",
    "            listkey.append(busqueda)    \n",
    "        except:\n",
    "            print(\"Sin link No cargar\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "110a9b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_doaj(busqueda):\n",
    "    #pasar a ingles \n",
    "    #funciona con el spacio \n",
    "    url = 'https://www.doaj.org/api/v2/search/articles/all('+busqueda+')?pageSize=100'\n",
    "    headers = {'Content-Type': 'application/json;charset=UTF-8', 'Access-Control-Allow-Origin': '*'}\n",
    "    r = requests.get(url, headers=headers)\n",
    "\n",
    "    #Consumo api  de doaj sobre titulo,url,abstract,author(uspto),keywords\n",
    "    content = r.json()\n",
    "    #Ingreso al contenido\n",
    "    json = content['results']\n",
    "\n",
    "    for i in range(len(json)):\n",
    "        try:\n",
    "            listalinks.append(json[i]['bibjson']['link'][0]['url'])\n",
    "            try:\n",
    "                listakeywords.append(json[i]['bibjson']['keywords'])\n",
    "            except:\n",
    "                listakeywords.append('0')\n",
    "\n",
    "            try:\n",
    "                listatitulos.append(json[i]['bibjson']['title'])\n",
    "            except:\n",
    "                listatitulos.append('0')\n",
    "            try:\n",
    "                listabstract.append(json[i]['bibjson']['abstract'])\n",
    "            except:\n",
    "                listabstract.append('0')\n",
    "            try:\n",
    "                aux_aut=[]\n",
    "                for j in range(len(json[i]['bibjson']['author'])):\n",
    "                    aux_aut.append(json[i]['bibjson']['author'][0]['name'])\n",
    "                listautor.append(list(set(aux_aut)))\n",
    "            except:\n",
    "                listautor.append('0')\n",
    "\n",
    "            listafuente.append(\"doaj\")\n",
    "            listkey.append(busqueda)\n",
    "        except:\n",
    "            print(\"Sin link No cargar\")\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "0a46cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_proquest(busqueda):\n",
    "\n",
    "    import xmltodict, json\n",
    "    access_token = '8461684a-5a79-407c-9321-93d8d719ddeb'\n",
    "    headers = {\n",
    "        'Content-type': 'text/xml',\n",
    "        'Authorization': 'Bearer {}'.format(access_token)\n",
    "    }\n",
    "    data = '<searchRequest><search><query>'+busqueda+'</query><databases><database>medlineprof</database></databases></search><count>100</count></searchRequest>'\n",
    "    resp = requests.post('https://api-dialog.proquest.com/v1/search', headers=headers, data=data)\n",
    "    o = xmltodict.parse(resp.text)\n",
    "    conta = json.dumps(o)\n",
    "    d = json.loads(conta)\n",
    "    \n",
    "\n",
    "    json = d['searchResponse']['result']['doc']\n",
    "    \n",
    "    for i in range(len(json)):\n",
    "            #print(\"************************************\")\n",
    "            #print(json[i])\n",
    "            try:\n",
    "                response2 = requests.get(json[i]['field'][4]['value'], headers=headers)\n",
    "                o1 = xmltodict.parse(response2.text)\n",
    "                listalinks.append(o1['Documents']['Literature']['DocInfo']['URL'])\n",
    "                import re\n",
    "                try:\n",
    "\n",
    "                    listatitulos.append(json[i]['field'][1]['value'])\n",
    "                except:\n",
    "                    listatitulos.append(\"0\")\n",
    "\n",
    "\n",
    "                try:\n",
    "                    nivel5= o1['Documents']['Literature']['Abstract']\n",
    "                    listabstract.append(nivel5.get('#text'))\n",
    "                except:\n",
    "                    listabstract.append(\"0\")\n",
    "                try:\n",
    "                    contributor=o1['Documents']['Literature']['Contributors']['Contributor']\n",
    "                    aux=list()\n",
    "                    #print(contributor)\n",
    "                    for i in range(len(contributor)):\n",
    "                        if(contributor[i]['@ContribRole']=='Author'):\n",
    "                            aux.append(contributor[i]['LastName'])\n",
    "                            \n",
    "                    listautor.append(aux)\n",
    "                except:\n",
    "                    listautor.append(\"0\")\n",
    "                try:\n",
    "                    keywords=o1['Documents']['Literature']['Subjects']['HeadingTerms']['HeadingTerm']\n",
    "                    aux=list()\n",
    "                    for i in range(len(keywords)):\n",
    "                        nivel9=keywords[i]['Heading']\n",
    "                        aux.append(nivel9['#text'])\n",
    "                    listakeywords.append(aux)\n",
    "                except:\n",
    "                    listakeywords.append('0')  \n",
    "\n",
    "                listafuente.append('ProQuest')\n",
    "                listkey.append(busqueda)\n",
    "            except:\n",
    "                print(\"no cargar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "81c18299",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_uspto(busqueda):\n",
    "    #api lenta \n",
    "    #funciona con spacio \n",
    "    #ingle\n",
    "    url_uspto = 'https://developer.uspto.gov/ibd-api/v1/application/publications?searchText='+busqueda+'&start=0&rows=100'\n",
    "    headers_uspto = {'Content-Type': 'application/json;charset=UTF-8', 'Access-Control-Allow-Origin': '*'}\n",
    "    r2_uspto = requests.get(url_uspto, headers=headers_uspto) \n",
    "    #Consumo api  de propiedad intelectual sobre titulo,url,abstract,nombre(uspto)\n",
    "    content_uspto = r2_uspto.json()\n",
    "    json_uspto = content_uspto['results']\n",
    "\n",
    "    for i in range(len(json_uspto)):\n",
    "        try:\n",
    "            listalinks.append(json_uspto[i]['filelocationURI'])\n",
    "            \n",
    "            try:\n",
    "                listatitulos.append(json_uspto[i]['inventionTitle'])\n",
    "            except:\n",
    "                listatitulos.append('0')\n",
    "\n",
    "            try:\n",
    "                listabstract.append(json_uspto[i]['abstractText'][0])\n",
    "            except:\n",
    "                listabstract.append(\"0\")\n",
    "            try:\n",
    "                listautor.append(json_uspto[i]['inventorNameArrayText'])\n",
    "            except:\n",
    "                listautor.append('0')\n",
    "\n",
    "            listafuente.append(\"uspto\")\n",
    "            listakeywords.append(0) \n",
    "            listkey.append(busqueda)\n",
    "        except:\n",
    "            print(\"Sin link No cargar\")\n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "56ce16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_ieeexplore(busqueda):\n",
    "    #https://www.google.co.uk/search?q=%22medicine%22&tbm=nws&gws_rd=ssl\n",
    "    url_ieeexplore = 'http://ieeexploreapi.ieee.org/api/v1/search/articles?apikey=3dyyvg36jv78dbuu83q3fzzt&format=json&max_records=100&start_record=1&sort_order=asc&sort_field=article_number&querytext='+busqueda\n",
    "    headers_ieeexplore = {'Content-Type': 'application/json;charset=UTF-8', 'Access-Control-Allow-Origin': '*'}\n",
    "    r_ieeexplore = requests.get(url_ieeexplore, headers=headers_ieeexplore)\n",
    "\n",
    "    content_ieeexplore = r_ieeexplore.json()\n",
    "    json_ieeexplore = content_ieeexplore['articles']\n",
    "    for i in range(len(json_ieeexplore)):\n",
    "        try:\n",
    "            listalinks.append(json_ieeexplore[i]['pdf_url'])\n",
    "            \n",
    "            try:\n",
    "                listabstract.append(json_ieeexplore[i]['abstract'])\n",
    "            except:\n",
    "                listabstract.append('0')\n",
    "            try:\n",
    "                listatitulos.append(json_ieeexplore[i]['title'])\n",
    "            except:\n",
    "                listatitulos.append('0')\n",
    "\n",
    "\n",
    "            try:\n",
    "                aux=list()\n",
    "                for j in range(len(json_ieeexplore[i]['authors']['authors'])):\n",
    "                    aux.append(json_ieeexplore[i]['authors']['authors'][j]['full_name'])\n",
    "                listautor.append(aux)\n",
    "            except:\n",
    "                listautor.append(\"0\")\n",
    "            try:\n",
    "                listakeywords.append(json_ieeexplore[i]['index_terms']['ieee_terms']['terms'])\n",
    "            except:\n",
    "                listakeywords.append('0')  \n",
    "            listafuente.append('ieeexplore')\n",
    "            listkey.append(busqueda)\n",
    "        except:\n",
    "            print(\"Sin link No cargar\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "dcc6542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consumo_google_noticas(busqueda):\n",
    "    from GoogleNews import GoogleNews\n",
    "    from newspaper import Article\n",
    "    import pandas as pd\n",
    "    import time\n",
    "    from random import randint\n",
    "    googlenews=GoogleNews(start='05/01/2020',end='19/06/2021')\n",
    "    googlenews.search(busqueda)\n",
    "    result=googlenews.result()\n",
    "    df=pd.DataFrame(result)\n",
    "\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            listalinks.append(df['link'].values[i])\n",
    "            try:\n",
    "                listatitulos.append(df['title'].values[i])\n",
    "            except:\n",
    "                listatitulos.append(\"0\")\n",
    "            try:\n",
    "                listabstract.append(df['desc'].values[i])\n",
    "            except:\n",
    "                listabstract.append(\"0\")\n",
    "            try:\n",
    "                aux=[]\n",
    "                aux.append(df['media'].values[i])\n",
    "                listautor.append(aux)\n",
    "            except:\n",
    "                listautor.append(\"0\")\n",
    "\n",
    "            listakeywords.append(\"0\")\n",
    "            listafuente.append(\"google\")\n",
    "            listkey.append(busqueda)\n",
    "        except:\n",
    "            print(\"Sin link No cargar\")\n",
    "            \n",
    "        \n",
    "    \n",
    "    #print('google:link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9c55fdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def categorizador_titlulo(listatitulos,name):\n",
    "    \n",
    "    import pandas as pd\n",
    "    import random\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    nlp = spacy.load('en')\n",
    " \n",
    "    for i in range(len(listatitulos)):\n",
    "        #print(\"Categoria Titulo:\",i)\n",
    "        doc = nlp(listatitulos[i])\n",
    "        list_token=[]\n",
    "        for token in doc:\n",
    "            if token.pos_=='ADJ' or token.pos_==\"NOUN\":\n",
    "                list_token.append(token.text)\n",
    "\n",
    "        list_token_palabra.append(list_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "16fe8e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def categorizador_abstract(listabstract,name):  \n",
    "    import pandas as pd\n",
    "    import random\n",
    "    import spacy\n",
    "    from spacy import displacy\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS\n",
    "    nlp = spacy.load('en')\n",
    "    \n",
    "    \n",
    "    for i in range(len(listabstract)):\n",
    "\n",
    "        doc = nlp(listabstract[i])\n",
    "        #print(\"Categoria Abstract:\",i)\n",
    "        list_token=[]\n",
    "        for token in doc:\n",
    "            if token.pos_=='ADJ' or token.pos_==\"NOUN\":\n",
    "                list_token.append(token.text)\n",
    "\n",
    "        list_token_palabra_abstract.append(list_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "3de53f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza(diccionario):\n",
    "    import pandas as pd  \n",
    "    dataset=pd.DataFrame(diccionario)\n",
    "    #print(dataset['link'].duplicated().sum())\n",
    "    #print(\"Numero de articulos con repetidos\",len(dataset))\n",
    "    dataset=dataset.drop_duplicates(subset=['link'])\n",
    "    dataset=dataset.drop_duplicates(subset=['abstract'])\n",
    "    #print(\"Numero de articulos sin repetidos\",len(dataset))\n",
    "    \n",
    "    #Trabajar con categorias en minusculas\n",
    "    dataset['keywords']= dataset.keywords.astype(str).str.lower()\n",
    "    dataset['authors']= dataset.authors.astype(str).str.lower()\n",
    "    dataset['categoriatitulo']= dataset.categoriatitulo.astype(str).str.lower()\n",
    "    dataset['categoriabstract']= dataset.categoriabstract.astype(str).str.lower()\n",
    "    \n",
    "    return dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1bb58917",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento: 0\n",
      "Palabra: Ullrich congenital muscular dystrophy\n",
      "link 7  title  7 abstract  7  authors  7  keywords  7 fuente 7\n",
      "link 7  title  7 abstract  7  authors  7  keywords  7 fuente 7\n",
      "link 18  title  18 abstract  18  authors  18  keywords  18 fuente 18\n",
      "link 118  title  118 abstract  118  authors  118  keywords  118 fuente 118\n",
      "link 120  title  120 abstract  120  authors  120  keywords  120 fuente 120\n",
      "Sin link No cargar\n",
      "link 129  title  129 abstract  129  authors  129  keywords  129 fuente 129\n",
      "Procesamiento: 1\n",
      "Palabra: obsolete JMP syndrome\n",
      "link 129  title  129 abstract  129  authors  129  keywords  129 fuente 129\n",
      "link 129  title  129 abstract  129  authors  129  keywords  129 fuente 129\n",
      "link 129  title  129 abstract  129  authors  129  keywords  129 fuente 129\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "link 131  title  131 abstract  131  authors  131  keywords  131 fuente 131\n",
      "Procesamiento: 2\n",
      "Palabra: obsolete Saldino-Noonan syndrome\n",
      "link 131  title  131 abstract  131  authors  131  keywords  131 fuente 131\n",
      "link 131  title  131 abstract  131  authors  131  keywords  131 fuente 131\n",
      "link 131  title  131 abstract  131  authors  131  keywords  131 fuente 131\n",
      "'NoneType' object is not iterable\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "link 131  title  131 abstract  131  authors  131  keywords  131 fuente 131\n",
      "Procesamiento: 3\n",
      "Palabra: liver carcinoma in situ\n",
      "link 231  title  231 abstract  231  authors  231  keywords  231 fuente 231\n",
      "link 231  title  231 abstract  231  authors  231  keywords  231 fuente 231\n",
      "link 247  title  247 abstract  247  authors  247  keywords  247 fuente 247\n",
      "link 347  title  347 abstract  347  authors  347  keywords  347 fuente 347\n",
      "link 351  title  351 abstract  351  authors  351  keywords  351 fuente 351\n",
      "link 361  title  361 abstract  361  authors  361  keywords  361 fuente 361\n",
      "Procesamiento: 4\n",
      "Palabra: obsolete hereditary sensory neuropathy\n",
      "link 361  title  361 abstract  361  authors  361  keywords  361 fuente 361\n",
      "link 361  title  361 abstract  361  authors  361  keywords  361 fuente 361\n",
      "link 361  title  361 abstract  361  authors  361  keywords  361 fuente 361\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "link 363  title  363 abstract  363  authors  363  keywords  363 fuente 363\n",
      "Procesamiento: 5\n",
      "Palabra: spondylocostal dysostosis\n",
      "link 382  title  382 abstract  382  authors  382  keywords  382 fuente 382\n",
      "link 382  title  382 abstract  382  authors  382  keywords  382 fuente 382\n",
      "link 382  title  382 abstract  382  authors  382  keywords  382 fuente 382\n",
      "link 382  title  382 abstract  382  authors  382  keywords  382 fuente 382\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "link 386  title  386 abstract  386  authors  386  keywords  386 fuente 386\n",
      "Procesamiento: 6\n",
      "Palabra: orofacial cleft\n",
      "link 459  title  459 abstract  459  authors  459  keywords  459 fuente 459\n",
      "link 459  title  459 abstract  459  authors  459  keywords  459 fuente 459\n",
      "link 515  title  515 abstract  515  authors  515  keywords  515 fuente 515\n",
      "link 515  title  515 abstract  515  authors  515  keywords  515 fuente 515\n",
      "link 525  title  525 abstract  525  authors  525  keywords  525 fuente 525\n",
      "Procesamiento: 7\n",
      "Palabra: obsolete West syndrome\n",
      "no cargar\n",
      "link 525  title  525 abstract  525  authors  525  keywords  525 fuente 525\n",
      "link 525  title  525 abstract  525  authors  525  keywords  525 fuente 525\n",
      "link 525  title  525 abstract  525  authors  525  keywords  525 fuente 525\n",
      "link 525  title  525 abstract  525  authors  525  keywords  525 fuente 525\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "Procesamiento: 8\n",
      "Palabra: obsolete Walker-Warburg syndrome\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "'NoneType' object is not iterable\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "Sin link No cargar\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "Procesamiento: 9\n",
      "Palabra: disorder of methionine catabolism\n",
      "no cargar\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "link 535  title  535 abstract  535  authors  535  keywords  535 fuente 535\n",
      "link 536  title  536 abstract  536  authors  536  keywords  536 fuente 536\n",
      "link 542  title  542 abstract  542  authors  542  keywords  542 fuente 542\n",
      "link 552  title  552 abstract  552  authors  552  keywords  552 fuente 552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_palabras = pd.read_csv('palabras.csv')\n",
    "\n",
    "#print(dataset_palabras['n.rdfs__label'])\n",
    "list_palabras_error=[]\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(\"Procesamiento:\",i)\n",
    "    print(\"Palabra:\",dataset_palabras['n.rdfs__label'][i])\n",
    "    busqueda = dataset_palabras['n.rdfs__label'][i]\n",
    "\n",
    "    \n",
    "    try:\n",
    "        consumo_proquest(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "    try:\n",
    "        #consumo_scopus(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "\n",
    "    try:\n",
    "        consumo_doaj(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "\n",
    "\n",
    "    try:\n",
    "        consumo_uspto(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "\n",
    "\n",
    "    try:\n",
    "        consumo_ieeexplore(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "    \n",
    "    try:\n",
    "        consumo_google_noticas(busqueda)\n",
    "        print('link',len(listalinks),' title ',len(listatitulos),'abstract ',len(listabstract),' authors ',len(listautor),\" keywords \",len(listakeywords),'fuente', len(listafuente))\n",
    "    except:\n",
    "        list_palabras_error.append(dataset_palabras['n.rdfs__label'][i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "23d41d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "#categorizador(listatitulos,listabstract)\n",
    "list_token_palabra_abstract=[]\n",
    "list_token_palabra=[]\n",
    "\n",
    "#CREAMOS PROCESOS A EJECUTAR EN PARALELO.        \n",
    "t = threading.Thread(target = categorizador_titlulo, args =(listatitulos, 'thread1') )\n",
    "t2 = threading.Thread(target = categorizador_abstract, args =(listabstract, 'thread2') )\n",
    "\n",
    "\n",
    "#INICIAMOS PROCESOS.\n",
    "t.start()\n",
    "t2.start()\n",
    "\n",
    "\n",
    "dic_general ={'link':listalinks ,'title':listatitulos,'abstract':listabstract,'authors':listautor,\"keywords\":listakeywords,'fuente':listafuente,'categoriatitulo':list_token_palabra,'categoriabstract':list_token_palabra_abstract,'key':listkey}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "24d35806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "552\n",
      "552\n",
      "552\n",
      "552\n",
      "552\n",
      "552\n",
      "552\n",
      "552\n"
     ]
    }
   ],
   "source": [
    "print(len(dic_general['link']))\n",
    "print(len(dic_general['title']))\n",
    "print(len(dic_general['abstract']))\n",
    "print(len(dic_general['authors']))\n",
    "print(len(dic_general['keywords']))\n",
    "print(len(dic_general['fuente']))\n",
    "print(len(dic_general['categoriatitulo']))\n",
    "print(len(dic_general['categoriabstract']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4230751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "3022d0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=limpieza(dic_general)\n",
    "dataset.to_csv('ResultadoCategoriaFinal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f68540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4de2713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de76ba40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267dee36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
